{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regular Expressions Lab\n",
    "\n",
    "Regular expressions (aka regexes or regexps) are text matching patterns described with a formal syntax that were covered in Lecture 3. This lab covers two main issues\n",
    "- Basic Python Capabilities for Using Regular Expressions\n",
    "- How to use regular expressions for a variety of tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Python Capabilities for Regular Expressions\n",
    "\n",
    "Regular expressions are implemented in the `re` package, which provides a number of functions to match regexps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found first term.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# List of patterns to search for\n",
    "\n",
    "# Text to parse\n",
    "text = 'This is a string with term1.'\n",
    "if re.search(\"term[0-4]\",  text):\n",
    "        print('Found first term.')\n",
    "if re.search(\"term[5-9]\",  text):\n",
    "        print ('Found second term.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've seen that `re.search()` will take the pattern, scan the text, and then returns a **Match** object corresponding to the first match. If no pattern is found, a **None** is returned. Both of these are truthy, so any successful match (even a match on a null string from an ill-advised Kleene \\*) will evaluate to `True` in a `Boolean` context. To give a clearer picture of this match object, check out the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The regexp matched '3225' between positions 16 and 20\n"
     ]
    }
   ],
   "source": [
    "match = re.search(\"\\d+\",  \"This is the COMP3225 module's 1st lab\")\n",
    "\n",
    "print(\"The regexp matched '%s' between positions %s and %s\" % (match.group(0), match.start(), match.end()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you used capture groups in the regular expression, they will appear as arguments `1` up to `99` of the `match.groups()` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The regexp matched '3225' and '1'\n"
     ]
    }
   ],
   "source": [
    "match = re.search(\"(\\d+).*(\\d+)\",  \"This is the COMP3225 module's 1st lab\")\n",
    "\n",
    "print(\"The regexp matched '%s' and '%s'\" % (match.group(1), match.group(2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three variants of the search function:\n",
    "- `re.match()` is anchored at the beginning of the search string\n",
    "- `re.fullmatch()` is anchored at the beginning and the end of the search string\n",
    "- `re.findall()` return all matches \n",
    "\n",
    "You can also look for all the matches in a string with `re.findall()`, but it returns a list of the actual strings matched rather than a `Match` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The regexp matched '['3225', '1']'\n"
     ]
    }
   ],
   "source": [
    "match = re.findall(\"\\d+\",  \"This is the COMP3225 module's 1st lab\")\n",
    "\n",
    "print(\"The regexp matched '%s'\" % (match))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To embed this in a _file-read-and-match-print-results_ code fragment that works line by line, we can do the following. We are using the codecs package to explicitly manage the various character sets that we might encounter.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '2021', '02380594479']\n",
      "['19865']\n",
      "['20', '49', '9']\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "fname=\"mytest.txt\"\n",
    "\n",
    "for line in codecs.open(fname,\"r\",encoding=\"utf-8\"):\n",
    "    match=re.findall(\"\\d+\", line)\n",
    "    if match: print(match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, there are four usefull flags you can use to turn on different features in Python's implementation of regular expressions. \n",
    "- VERBOSE. Allow inline comments and extra whitespace.\n",
    "- IGNORECASE. Do case-insensitive matches.\n",
    "- DOTALL. Allow dot (.) to match any character, including a newline. (The default behavior of dot is to match anything, except for a newline.)\n",
    "- MULTILINE. Allow anchors (^ and $) to match the beginnings and ends of lines instead of matching the beginning and end of the whole text.\n",
    "\n",
    "They can be provided as flags to the re methods\n",
    "- re.match(\"this\", \"This\", flags=re.IGNORECASE)\n",
    "\n",
    "or as abbreviated flags\n",
    "- re.match(\"that\", \"That\", flags=re.I)\n",
    "\n",
    "or as inline flags in the regular expression itself\n",
    "- re.match(\"(?i)those\", \"Those\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practicing Regular Expressions\n",
    "\n",
    "You should now have a good enough understanding of how to use the regular expressions module in Python to undertake the rest of the tasks in the lab. You should refer to the full [Python regular expression documentation](https://docs.python.org/3/library/re.html)  for more detail.\n",
    "\n",
    "Note that so far we haven't needed to use multiline or raw Python strings, or verbose regular expressions. As things get more complex, you might need to.\n",
    "\n",
    "The following six tasks consist of three skills tasks and three application tasks. You should attempt all the skills tasks (1-3) to your satisfaction, and choose any of the application tasks (4-6) for general practice in using regular expressions to solve problems that require complex lexical analysis.\n",
    "\n",
    "Please note: there are no right answers set for these tasks - no specific precision or recall targets that you have to meet. The intention is that you focus on practicing the use of regular expressions, and gain experience with applying different kinds of \"patterns\" to a range of situations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 - recognising specific lexical tokens\n",
    "Recognise the following particular kinds of non-word lexical tokens, using your experience to define the allowable format of each token type. A whole load of data from the internet (including some HTML code) has been put into an example test file for you.\n",
    "\n",
    "1. Hashtags (which can contain alphanumeric characters, underscores and hyphens). Test on the text of some tweets that you copy and paste from twitter.com.\n",
    "1. UK Postcodes like SO17 1BJ or E1 8BN.\n",
    "1. UK style phone numbers like 023 80591234 or 02380 590000 or +44 (0) 23 80594479 or +44 (23) 8059 5000 or mobile numbers like 07722 175921.\n",
    "1. URLs http://google.com/ or https://a.b.net/c/d/e.php#fragment.\n",
    "1. Email addresses like lac@ecs.soton.ac.uk, and decompose into username and internet domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 :  [('example', 'gmail.co.uk')]\n",
      "2 :  ['+44 (0) 23']\n",
      "3 :  ['+44 (23)']\n",
      "7 :  ['#CoronaVirus', '#Covid19', '#selfquarantine', '#NHS111']\n",
      "7 :  ['https://t.co/8dSjEKoacz']\n",
      "8 :  ['#coronavirus', '#Advice', '#NHS', '#Lincolnshire', '#Health', '#Care']\n",
      "8 :  ['https://t.co/F3xzAY7aNA.', 'https://t.co/JLyp7KtI7E', 'https://t.co/h']\n",
      "9 :  ['#CoronaVirus', '#COVID19', '#NHS111']\n",
      "9 :  ['https://t.co/WtPyjdoFq9']\n",
      "17 :  ['MK44 1QU']\n",
      "61 :  ['AB11 5BY']\n",
      "72 :  ['OX14 3HG']\n",
      "94 :  ['IP15 5DS']\n",
      "99 :  ['AB10 7QB', 'AB10 7QB']\n",
      "100 :  ['AB24 3FX', 'AB24 3FX']\n",
      "106 :  ['MK43 0AL', 'MK43 0AL']\n",
      "108 :  ['MK18 1EG', 'MK18 1EG']\n",
      "109 :  ['HP11 2JZ', 'HP11 2JZ']\n",
      "114 :  ['TR11 4RH', 'TR11 4RH']\n",
      "117 :  ['BT52 1SA', 'BT52 1SA']\n",
      "119 :  ['DE22 1GB', 'DE22 1GB']\n",
      "122 :  ['BH12 5HH', 'BH12 5HH']\n",
      "123 :  ['BH12 5BB', 'BH12 5BB']\n",
      "124 :  ['SY23 2AX', 'SY23 2AX']\n",
      "128 :  ['GL52 2RH', 'GL52 2RH']\n",
      "129 :  ['LL57 2DG', 'LL57 2DG']\n",
      "130 :  ['SO14 0YN', 'SO14 0YN']\n",
      "130 :  ['+44 (23)']\n",
      "131 :  ['SO17 1BJ', 'SO17 1BJ']\n",
      "131 :  ['+44 (23)']\n",
      "132 :  ['SO22 4NR', 'SO22 4NR']\n",
      "133 :  ['AL10 9AB', 'AL10 9AB']\n",
      "137 :  ['DA15 9DF', 'DA15 9DF']\n",
      "152 :  ['LE11 3TU', 'LE11 3TU']\n",
      "169 :  ['SE10 0EW', 'SE10 0EW']\n",
      "170 :  ['SE10 9JF', 'SE10 9JF']\n",
      "171 :  ['SE10 9LS', 'SE10 9LS']\n",
      "178 :  ['SE14 6NW', 'SE14 6NW']\n",
      "180 :  ['SW17 0RE', 'SW17 0RE']\n",
      "189 :  ['EH11 4BN', 'EH11 4BN']\n",
      "190 :  ['EH14 4AS', 'EH14 4AS']\n",
      "193 :  ['EH21 6UU', 'EH21 6UU']\n",
      "197 :  ['YO10 5DD', 'YO10 5DD']\n",
      "202 :  ['TF10 8NB', 'TF10 8NB']\n",
      "204 :  ['CF10 3XP', 'CF10 3XP']\n",
      "209 :  ['TW20 0EX', 'TW20 0EX']\n",
      "213 :  ['TW10 6JP', 'TW10 6JP']\n",
      "224 :  ['PO19 6PE', 'PO19 6PE']\n",
      "230 :  ['LS18 5HD', 'LS18 5HD']\n",
      "233 :  [('patrick.grady.mp', 'parliament.uk')]\n",
      "234 :  [('david.linden.mp', 'parliament.uk')]\n",
      "235 :  [('stewart.mcdonald.mp', 'parliament.uk')]\n",
      "236 :  [('anne.mclaughlin.mp', 'parliament.uk')]\n",
      "237 :  [('carol.monaghan.mp', 'parliament.uk')]\n",
      "238 :  [('chris.stephens.mp', 'parliament.uk')]\n",
      "239 :  [('alison.thewliss.mp', 'parliament.uk')]\n",
      "254 :  [('kerry.mccarthy.mp', 'parliament.uk')]\n",
      "263 :  [('darren.jones.mp', 'parliament.uk')]\n",
      "272 :  [('karin.smyth.mp', 'parliament.uk')]\n",
      "281 :  [('thangam.debbonaire.mp', 'parliament.uk')]\n",
      "286 :  ['http://soundgardenworld.com', 'https://www.phoenixstudios.co.uk/blog/wp']\n",
      "290 :  ['http://www.fivefingerdeathpunch.com', 'https://www.phoenixstudios.co.uk/blog/wp']\n",
      "294 :  ['http://www.suiepaparude.ro', 'https://www.phoenixstudios.co.uk/blog/wp']\n",
      "298 :  ['http://www.rollingstones.com', 'https://www.phoenixstudios.co.uk/blog/wp']\n",
      "302 :  ['http://www.rihannanow.com/rihanna', 'https://www.phoenixstudios.co.uk/blog/wp']\n",
      "306 :  ['http://www.pendulum.com', 'https://www.phoenixstudios.co.uk/blog/wp']\n",
      "310 :  ['http://www.oasisinet.com', 'https://www.phoenixstudios.co.uk/blog/wp']\n",
      "314 :  ['http://www.nin.com/index', 'https://www.phoenixstudios.co.uk/blog/wp']\n",
      "316 :  ['https://www.phoenixstudios.co.uk/contact']\n",
      "319 :  ['http://muse.mu', 'https://www.phoenixstudios.co.uk/blog/wp']\n",
      "323 :  ['http://whoismgmt.com/visualize', 'https://www.phoenixstudios.co.uk/blog/wp']\n",
      "327 :  ['http://www.linkinpark.com', 'https://www.phoenixstudios.co.uk/blog/wp']\n",
      "331 :  ['http://lifeandtimes.com', 'https://www.phoenixstudios.co.uk/blog/wp']\n",
      "335 :  ['http://www.ironmaiden.com', 'https://www.phoenixstudios.co.uk/blog/wp']\n",
      "339 :  ['http://www.hangingupthemoon.com/index.php', 'https://www.phoenixstudios.co.uk/blog/wp']\n",
      "343 :  ['http://gunsnroses.com', 'https://www.phoenixstudios.co.uk/blog/wp']\n",
      "347 :  ['http://www.fleetwoodmac.com', 'https://www.phoenixstudios.co.uk/blog/wp']\n",
      "351 :  ['http://www.daftpunk.com', 'https://www.phoenixstudios.co.uk/blog/wp']\n",
      "355 :  ['http://www.beyonce.com', 'https://www.phoenixstudios.co.uk/blog/wp']\n",
      "359 :  ['http://www.acdc.com/us/home', 'https://www.phoenixstudios.co.uk/blog/wp']\n",
      "363 :  ['http://www.jimihendrix.com/us/home', 'https://www.phoenixstudios.co.uk/blog/wp']\n",
      "365 :  ['http://eepurl.com/SBoUf']\n",
      "368 :  ['#nhs111', '#communitytransmission', '#coronavirus', '#COVID19']\n",
      "368 :  ['https://t.co/glu']\n",
      "369 :  ['#CoronaVirusUpdate', '#CoronaVirusUpdatesireland', '#VirusCorona', '#washdontwalk', '#NHS111', '#italylockdown', '#ForzaInter', '#PopeFrancis', '#massviavideolink', '#coronavirus', '#coronafizzypop', '#crestafizzypop', '#crestavirusnext']\n",
      "369 :  ['https://t.co/EGe45zqzbL']\n",
      "370 :  ['#NHS111', '#coronavirus']\n",
      "370 :  ['https://t.co/6fZ0VImlmV']\n",
      "371 :  ['#coronavirus']\n",
      "372 :  ['#NHS111', '#Tuesday', '#Coronavirus', '#NHS']\n",
      "372 :  ['https://t.co/fu']\n",
      "373 :  ['#NHS111']\n",
      "374 :  ['#BorisJohnson', '#MattHancock', '#coronavirus', '#CoronaVirusUpdate', '#coronavirusuk', '#covid19UK', '#COVID19', '#NHS111', '#NHS']\n",
      "374 :  ['https://t.co/3cYUQikFH2']\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "fname=\"misctext.txt\"\n",
    "hashtag_re=\"#\\w*\"\n",
    "postcode_re=\"[A-Z]{2}\\d{2}\\s\\d[A-Z]{2}\"\n",
    "phone_re=\"(?:\\+44\\s\\(23\\))|(?:\\+44\\s\\(0\\)\\s23)\"\n",
    "url_re=\"(?:http|https)://[A-Za-z.0-9_]+.[a-z.]+(?:/[a-zA-Z0-9._#/]+)*\"\n",
    "email_re=\"([a-zA-Z0-9.]+)@([a-zA-Z.]*\\.[a-zA-Z]+)\"\n",
    "\n",
    "\n",
    "count = 0\n",
    "for line in codecs.open(fname,\"r\",encoding=\"utf-8\"):\n",
    "    count += 1\n",
    "    match=re.findall(hashtag_re, line)\n",
    "    if match: print(count, \": \", match)\n",
    "    match=re.findall(postcode_re, line)\n",
    "    if match: print(count, \": \", match)\n",
    "    match=re.findall(phone_re, line)\n",
    "    if match: print(count, \": \", match)\n",
    "    match=re.findall(url_re, line)\n",
    "    if match: print(count, \": \", match)\n",
    "    match=re.findall(email_re, line)\n",
    "    if match:\n",
    "        print(count, \": \", match)\n",
    "        \n",
    "    #etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If that was too easy for you, look up the rules for the officially allowable formats of each token type in Wikipedia. Here for example, are the official UK postcode formats:\n",
    "\n",
    "| Format\t    | Coverage\t| Example |\n",
    "| -------:      | :--------:  | :------- |\n",
    "| AA9A 9AA\t| WC postcode area; EC1–EC4, NW1W, SE1P, SW1\t| EC1A 1BB |\n",
    "| A9A 9AA\t| E1, N1, W1\t| W1A 0AX |\n",
    "| A9 9AA\t| B, E, G, L, M, N, S, W\t| M1 1AE |\n",
    "| A99 9AA\t| \" | B33 8TH |\n",
    "| AA9 9AA\t| All other postcodes\t| CR2 6XH  |\n",
    "| AA99 9AA\t| \" | DN55 1PT |\n",
    "\n",
    "The rules for email adddresses are eye-watering!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - numeric tokens\n",
    "\n",
    "The zip file [guardian.zip](../corpus/guardian.zip) contains the text extracted from 118 Guardian news stories from the last year about Southampton, Portsmouth and Winchester. (One story per UTF-8 file, but you might like to combine them into a single file for ease of processing.) Starting from the regexp tokenizer example in Fig 2.12 (reproduced below), extend the set of tokens recognised to capture the following types of numeric data.\n",
    "- Numbers: -12  47.2  74,832,101\n",
    "- Time: 09:17pm\n",
    "- Money: £27.8m £8bn\n",
    "- Length: 6ft 48cm\n",
    "- Phone: +44(0)2380594479\n",
    "- Age specification: 13-year-old\n",
    "- Percentage: 14.4%\n",
    "- Temperature: 28C\n",
    "- Ordinals: 48th 22nd 1st\n",
    "\n",
    "You can compare your results with the [list of numeric tokens that I managed to extract](../corpus/guardian-numerics.txt). What’s the biggest financial quantity that appears in these stories? What did it relate to? What are the most common numeric tokens, and why do they appear?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['That', 'U.S.A.', 'poster-print', 'costs', '$12.40', '...']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "text='That U.S.A. poster-print costs $12.40...'\n",
    "\n",
    "# The following pattern is reproduced from the textbook figure 2.12.\n",
    "# UNFORTUNATELY, the behaviour of NLTK has changed since version 3.1,\n",
    "# so that capture groups don't work any more and every set\n",
    "# of grouping parentheses () has now to explicitly declare non-capturing semantics with ?:\n",
    "pattern = r'''(?x)          # set flag to allow verbose regexps \n",
    "\t (?:[A-Z]\\.)+\t\t\t# abbreviations, e.g. U.S.A. \n",
    "\t | \\w+(?:-\\w+)*\t\t\t# words with optional internal hyphens \n",
    "\t | \\$?\\d+(?:\\.\\d+)?%?\t# currency and percentages, e.g. $12.40, 82% \n",
    "\t | \\.\\.\\.\t\t\t\t# ellipsis \n",
    "\t | [][.,;\"'?():-_`]\t\t# these are separate punctuation tokens; includes ], [ \n",
    "\t '''\n",
    "\n",
    "tokens=nltk.regexp_tokenize(text, pattern)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 - lookahead and lookbehind assertions\n",
    "Use the lookaround capabilities to match the following (see the lookaround slide in lecture 3 as a start)\n",
    "- a password with at least 6 characters, containing 2 upper case letters, two digits and two punctuation marks\n",
    "- the above password, but it can't start with AB, ab, 01 or 12 to trap the really obvious abcd... and 1234... passwords\n",
    "- separate out the different components of a camelCase word (e.g. getElementById -> [get, Element, by, Id])\n",
    "-- Hint: identify each location where the previous character is lowercase and the next character is uppercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\".PassworD.99\"\n",
    "\n",
    "pattern = r'''(?x)\n",
    "    ()\n",
    "\n",
    "\n",
    "\n",
    "#code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Task 4 - Analysing Lightbulb Jokes\n",
    "There were more than 10k lightbulb jokes told on Twitter in in 2015. Here are a couple of examples of the genre.\n",
    "- Q: How many thriller writers does it take to change a lightbulb? A: Two. One to screw it almost all the way in and another to give a surprising twist at the end.\n",
    "- Do you know how many folk musicians it takes to change a lightbulb? Five. One to change the lightbulb, and four to write songs about how much better the old bulb was.\n",
    "\n",
    "The standard pattern for the opening of a lightbulb joke is _How many X does it take to change a lightbulb?_\n",
    "\n",
    "The file [lightbulbs-2015.txt](../corpus/lightbulbs-2015.txt) contains one lightbulb joke tweet per line.\n",
    "Write a python code that uses regular expressions to isolates the topic X of each of the jokes and use it to produce a summary of the top 100 topics of lightbulb humour.\n",
    "- You should throw away the answers (punchlines) to the joke, just look for the topic\n",
    "- Hint: you will need to use trial and error to deal with variations in language, case and punctuation. You may want to test sets of regular expressions in an editor such as VI to allow you to see the coverage and special cases before you commit them to python code.\n",
    "- This is unmoderated Internet gathered data. Apologies for any inappropriate language that it might contain, or any examples of offensive humour. If you find anything that should be removed, please let me know.\n",
    "\n",
    "The file [lightbulbs-2020.txt](../corpus/lightbulbs-2020.txt) contains the same data for last year. Use the regular expressions you developed to generate a top 100 summary for 2020 and compare the two years. What significant changes in topics have there been between 2015 and 2020?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The regexp matched 0 jokes out of 10042\n"
     ]
    }
   ],
   "source": [
    "#download the file beforehand\n",
    "fname=\"../corpus/lightbulbs-2015.txt\"\n",
    "\n",
    "#make the right regular expression to isolate just the topic\n",
    "pattern=\"what (.*) what\"\n",
    "\n",
    "lineN=0; matchN=0\n",
    "for line in open(fname,\"r\"):\n",
    "    lineN+=1\n",
    "    match=re.search(pattern, line)\n",
    "    if match:\n",
    "        matchN+=1\n",
    "        print(match.group(1))\n",
    "print(\"The regexp matched %s jokes out of %s\" % (matchN, lineN)) \n",
    "\n",
    "#now go on and do the analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Task 5 - citations\n",
    "\n",
    "The file [thesis.txt](https://secure.ecs.soton.ac.uk/notes/comp3225/labs/citations-thesis.txt) contains the text of a PhD thesis. The examiner wants to know how many papers the student has cited, and of those, how many are “recent” (i.e. from the last decade).\n",
    "The bibliographic style used means that citations appear in the text with the basic form (surnames year), and you can see examples of this as they appear in the PDF <img src=\"https://secure.ecs.soton.ac.uk/notes/comp3225/labs/citations-sample.png\" width=\"400\">\n",
    "\n",
    "Write regular expressions to extract the list of citations from the thesis, answering the examiner's questions above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional Task 6 - ELIZA\n",
    "\n",
    "ELIZA was an early natural language processing computer program created in 1964 (the year I was born) at the MIT Artificial Intelligence Laboratory and created to demonstrate superficial communication techniques (OMG - it's like we're twins).\n",
    "\n",
    "Eliza simulated conversation by using a \"pattern matching\" and substitution methodology that gave users an illusion of understanding on the part of the program. Famously ELIZA simulated a style of psychotherapist, well-known for simply parroting back at patients what they had just said, and used rules to respond with non-directional questions to user inputs. As such, ELIZA was one of the first chatterbots and one of the first programs capable of attempting the Turing test.\n",
    "\n",
    "You should implement an ELIZA-like conversational program, using substitutions such as those described on page 11 in the text book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
